{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pytorch_lightning as pl\n","import torch_geometric\n","import wandb\n","from pytorch_lightning.loggers import WandbLogger\n","import time\n","import torch\n","from smtr_e3nn_model import SMTR, create_transform, Ligand_dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["learning_rate = 1e-5\n","max_epoch = 1\n","if_new_train = True\n","check_point = '20220516_114821.ckpt'\n","accelerator = 'cpu'\n","if torch.cuda.is_available():\n","    accelerator = 'gpu'\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["train_dataset = '../dataset/fda_train_new.txt'\n","test_dataset = '../dataset/fda_test_new.txt'\n","transform = create_transform()\n","# data\n","train_dataset = Ligand_dataset(train_dataset, transform=transform)\n","test_dataset = Ligand_dataset(test_dataset, transform=transform)\n","train_dataloader = torch_geometric.loader.DataLoader(train_dataset, shuffle=True)\n","test_dataloader = torch_geometric.loader.DataLoader(test_dataset, shuffle=True)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuabei\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"http://localhost:8080/huabei/smtr_e3nn/runs/vdo1cp7i\" target=\"_blank\">wobbly-water-22</a></strong> to <a href=\"http://localhost:8080/huabei/smtr_e3nn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["if if_new_train:\n","    # file_position = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n","    # os.mkdir(file_position)\n","    smnn = SMTR(learning_rate=learning_rate)\n","else:\n","    smnn = SMTR().load_from_checkpoint(checkpoint_path=check_point)\n","project = 'smtr_e3nn'\n","wandb.init(project=project)\n","wandb_logger = WandbLogger(save_dir='.')\n","trainer = pl.Trainer(logger=wandb_logger, max_epochs=max_epoch,\n","                        auto_scale_batch_size=True, accelerator='cpu', devices=1)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","   | Name     | Type           | Params\n","---------------------------------------------\n","0  | elu      | ELU            | 0     \n","1  | lin1     | Linear         | 192   \n","2  | conv10   | Convolution    | 7.5 K \n","3  | conv11   | Convolution    | 7.0 K \n","4  | conv12   | Convolution    | 7.0 K \n","5  | norm10   | Norm           | 0     \n","6  | norm11   | Norm           | 0     \n","7  | norm12   | Norm           | 0     \n","8  | lin20    | Linear         | 576   \n","9  | lin21    | Linear         | 576   \n","10 | lin22    | Linear         | 576   \n","11 | nonlin10 | NormActivation | 24    \n","12 | nonlin11 | NormActivation | 24    \n","13 | nonlin12 | NormActivation | 24    \n","14 | lin30    | Linear         | 288   \n","15 | lin31    | Linear         | 288   \n","16 | lin32    | Linear         | 288   \n","17 | conv2    | ModuleDict     | 27.8 K\n","18 | norm20   | Norm           | 0     \n","19 | norm21   | Norm           | 0     \n","20 | norm22   | Norm           | 0     \n","21 | lin40    | Linear         | 432   \n","22 | lin41    | Linear         | 864   \n","23 | lin42    | Linear         | 864   \n","24 | nonlin20 | NormActivation | 12    \n","25 | nonlin21 | NormActivation | 12    \n","26 | nonlin22 | NormActivation | 12    \n","27 | lin50    | Linear         | 48    \n","28 | lin51    | Linear         | 48    \n","29 | lin52    | Linear         | 48    \n","30 | conv3    | ModuleDict     | 3.9 K \n","31 | norm30   | Norm           | 0     \n","32 | norm31   | Norm           | 0     \n","33 | norm32   | Norm           | 0     \n","34 | lin60    | Linear         | 48    \n","35 | lin61    | Linear         | 96    \n","36 | lin62    | Linear         | 96    \n","37 | nonlin30 | NormActivation | 4     \n","38 | nonlin31 | NormActivation | 4     \n","39 | nonlin32 | NormActivation | 4     \n","40 | dense1   | Linear         | 20    \n","41 | dense2   | Linear         | 1.3 K \n","42 | dense3   | Linear         | 257   \n","---------------------------------------------\n","60.2 K    Trainable params\n","0         Non-trainable params\n","60.2 K    Total params\n","0.241     Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e571d971edf4cb3956b78bfaadd0319","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 28. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 20. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef3423c8324f41a9ba7460d9bd4950f3","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89eebe682fac4f95973b2b6847f9e925","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 19. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 21. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 18. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 22. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 33. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 46. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 31. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 40. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 26. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 39. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 27. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 13. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 35. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 11. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 36. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 14. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 29. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 34. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 65. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 17. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 9. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 43. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 41. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 49. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 38. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 37. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 52. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 45. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 42. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 50. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n"]}],"source":["trainer.fit(smnn, train_dataloader, test_dataloader)\n","trainer.save_checkpoint(time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + \".ckpt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"a75462293d05fc3e00128f4985dd13fcf50f4f5144b1474848efbcac1f09cd24"},"kernelspec":{"display_name":"Python 3.8.12 ('smtr')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
