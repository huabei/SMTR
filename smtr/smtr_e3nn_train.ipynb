{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pytorch_lightning as pl\n","import torch_geometric\n","import wandb\n","from pytorch_lightning.loggers import WandbLogger\n","import time\n","import torch\n","from smtr_e3nn_model import SMTR, create_transform, Ligand_dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["learning_rate = 1e-5\n","max_epoch = 5\n","if_new_train = True\n","check_point = '20220516_114821.ckpt'\n","accelerator = 'cpu'\n","if torch.cuda.is_available():\n","    accelerator = 'gpu'\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["train_dataset = '../dataset/data_train.txt'\n","val_dataset = '../dataset/data_test.txt'\n","transform = create_transform()\n","# data\n","train_dataset = Ligand_dataset(train_dataset, transform=transform)\n","val_dataset = Ligand_dataset(val_dataset, transform=transform)\n","train_dataloader = torch_geometric.loader.DataLoader(train_dataset)\n","val_dataloader = torch_geometric.loader.DataLoader(val_dataset)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuabei\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"http://localhost:8080/huabei/smtr_e3nn/runs/t7lzhoag\" target=\"_blank\">efficient-oath-7</a></strong> to <a href=\"http://localhost:8080/huabei/smtr_e3nn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n"]},{"ename":"MisconfigurationException","evalue":"GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)","\u001b[1;32m/home/huabei/Projects/smtr/smtr/smtr_e3nn_train.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/smtr/smtr/smtr_e3nn_train.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39mproject)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/smtr/smtr/smtr_e3nn_train.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m wandb_logger \u001b[39m=\u001b[39m WandbLogger(save_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/smtr/smtr/smtr_e3nn_train.ipynb#ch0000004vscode-remote?line=9'>10</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(logger\u001b[39m=\u001b[39;49mwandb_logger, max_epochs\u001b[39m=\u001b[39;49mmax_epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/smtr/smtr/smtr_e3nn_train.ipynb#ch0000004vscode-remote?line=10'>11</a>\u001b[0m                         auto_scale_batch_size\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accelerator\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m'\u001b[39;49m, devices\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n","File \u001b[0;32m~/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=335'>336</a>\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=337'>338</a>\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=338'>339</a>\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=479'>480</a>\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=480'>481</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=482'>483</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=483'>484</a>\u001b[0m     num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=484'>485</a>\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=485'>486</a>\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=486'>487</a>\u001b[0m     ipus\u001b[39m=\u001b[39;49mipus,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=487'>488</a>\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=488'>489</a>\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=489'>490</a>\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=490'>491</a>\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=491'>492</a>\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=492'>493</a>\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=493'>494</a>\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49mreplace_sampler_ddp,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=494'>495</a>\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=495'>496</a>\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49mauto_select_gpus,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=496'>497</a>\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=497'>498</a>\u001b[0m     amp_type\u001b[39m=\u001b[39;49mamp_backend,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=498'>499</a>\u001b[0m     amp_level\u001b[39m=\u001b[39;49mamp_level,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=499'>500</a>\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=500'>501</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=501'>502</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=502'>503</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:196\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=193'>194</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=194'>195</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_accelerator()\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=195'>196</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_parallel_devices_and_init_accelerator()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=197'>198</a>\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=198'>199</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_and_init_cluster_environment()\n","File \u001b[0;32m~/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:514\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=509'>510</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=510'>511</a>\u001b[0m     available_accelerator \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=511'>512</a>\u001b[0m         acc_str \u001b[39mfor\u001b[39;00m acc_str \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types \u001b[39mif\u001b[39;00m AcceleratorRegistry\u001b[39m.\u001b[39mget(acc_str)\u001b[39m.\u001b[39mis_available()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=512'>513</a>\u001b[0m     ]\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=513'>514</a>\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=514'>515</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m can not run on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=515'>516</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=516'>517</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=517'>518</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer`: \u001b[39m\u001b[39m{\u001b[39;00mavailable_accelerator\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=518'>519</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=520'>521</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=522'>523</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus\n","\u001b[0;31mMisconfigurationException\u001b[0m: GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."]}],"source":["if if_new_train:\n","    # file_position = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n","    # os.mkdir(file_position)\n","    smnn = SMTR(learning_rate=learning_rate)\n","else:\n","    smnn = SMTR().load_from_checkpoint(checkpoint_path=check_point)\n","project = 'smtr_e3nn'\n","wandb.init(project=project)\n","wandb_logger = WandbLogger(save_dir='.')\n","trainer = pl.Trainer(logger=wandb_logger, max_epochs=max_epoch,\n","                        auto_scale_batch_size=True, accelerator='cpu', devices=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.fit(smnn, train_dataloader, val_dataloader)\n","trainer.save_checkpoint(time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + \".ckpt\")"]}],"metadata":{"interpreter":{"hash":"a75462293d05fc3e00128f4985dd13fcf50f4f5144b1474848efbcac1f09cd24"},"kernelspec":{"display_name":"Python 3.8.12 ('smtr')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
