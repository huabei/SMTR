{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pytorch_lightning as pl\n","import torch_geometric\n","import wandb\n","from pytorch_lightning.loggers import WandbLogger\n","import time\n","from smtr_e3nn_model import SMTR, create_transform, Ligand_dataset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["learning_rate = 1e-5\n","max_epoch = 5\n","if_new_train = True\n","check_point = '20220516_114821.ckpt'\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.16"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/mnt/e/Python_Project/SMTR/smtr/wandb/run-20220519_105213-31bty1t9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"http://localhost:8080/huabei/smtr_e3nn/runs/31bty1t9\" target=\"_blank\">young-wildflower-4</a></strong> to <a href=\"http://localhost:8080/huabei/smtr_e3nn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n"]}],"source":["project = 'smtr_e3nn'\n","wandb.init(project=project)\n","wandb_logger = WandbLogger(save_dir='.')\n","train_dataset = '../dataset/data_train.txt'\n","val_dataset = '../dataset/data_test.txt'\n","transform = create_transform()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","# data\n","train_dataset = Ligand_dataset(train_dataset, transform=transform)\n","val_dataset = Ligand_dataset(val_dataset, transform=transform)\n","train_dataloader = torch_geometric.loader.DataLoader(train_dataset, batch_size=8)\n","val_dataloader = torch_geometric.loader.DataLoader(val_dataset, batch_size=8)\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.\n","  rank_zero_deprecation(\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}],"source":["if if_new_train:\n","    # file_position = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n","    # os.mkdir(file_position)\n","    smnn = SMTR(learning_rate=learning_rate)\n","else:\n","    smnn = SMTR().load_from_checkpoint(checkpoint_path=check_point)\n","trainer = pl.Trainer(logger=wandb_logger, max_epochs=max_epoch, accumulate_grad_batches=2, terminate_on_nan=True,\n","                        auto_scale_batch_size=True, accelerator='gpu', devices=1)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name     | Type           | Params\n","---------------------------------------------\n","0  | elu      | ELU            | 0     \n","1  | lin1     | Linear         | 192   \n","2  | conv10   | Convolution    | 7.5 K \n","3  | conv11   | Convolution    | 7.0 K \n","4  | conv12   | Convolution    | 7.0 K \n","5  | norm10   | Norm           | 0     \n","6  | norm11   | Norm           | 0     \n","7  | norm12   | Norm           | 0     \n","8  | lin20    | Linear         | 576   \n","9  | lin21    | Linear         | 576   \n","10 | lin22    | Linear         | 576   \n","11 | nonlin10 | NormActivation | 24    \n","12 | nonlin11 | NormActivation | 24    \n","13 | nonlin12 | NormActivation | 24    \n","14 | lin30    | Linear         | 288   \n","15 | lin31    | Linear         | 288   \n","16 | lin32    | Linear         | 288   \n","17 | conv2    | ModuleDict     | 27.8 K\n","18 | norm20   | Norm           | 0     \n","19 | norm21   | Norm           | 0     \n","20 | norm22   | Norm           | 0     \n","21 | lin40    | Linear         | 432   \n","22 | lin41    | Linear         | 864   \n","23 | lin42    | Linear         | 864   \n","24 | nonlin20 | NormActivation | 12    \n","25 | nonlin21 | NormActivation | 12    \n","26 | nonlin22 | NormActivation | 12    \n","27 | lin50    | Linear         | 48    \n","28 | lin51    | Linear         | 48    \n","29 | lin52    | Linear         | 48    \n","30 | conv3    | ModuleDict     | 3.9 K \n","31 | norm30   | Norm           | 0     \n","32 | norm31   | Norm           | 0     \n","33 | norm32   | Norm           | 0     \n","34 | lin60    | Linear         | 48    \n","35 | lin61    | Linear         | 96    \n","36 | lin62    | Linear         | 96    \n","37 | nonlin30 | NormActivation | 4     \n","38 | nonlin31 | NormActivation | 4     \n","39 | nonlin32 | NormActivation | 4     \n","40 | dense1   | Linear         | 1.3 K \n","41 | dense2   | Linear         | 9.5 K \n","42 | dense3   | Linear         | 257   \n","---------------------------------------------\n","69.7 K    Trainable params\n","0         Non-trainable params\n","69.7 K    Total params\n","0.279     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           "]},{"name":"stderr","output_type":"stream","text":["/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 191. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 139. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n","/home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0:  29%|██▉       | 3920/13486 [26:21<1:04:20,  2.48it/s, loss=3.18, v_num=y1t9] "]},{"ename":"ValueError","evalue":"The loss returned in `training_step` is nan.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/home/huabei/Projects/SMTR/smtr/smtr_e3nn.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTR/smtr/smtr_e3nn.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(smnn, train_dataloader, val_dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTR/smtr/smtr_e3nn.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_checkpoint(time\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mlocaltime()) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.ckpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/huabei/Projects/SMTR/smtr/smtr_e3nn.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=804'>805</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1348'>1349</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1349'>1350</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1350'>1351</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:268\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=263'>264</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=206'>207</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=207'>208</a>\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=209'>210</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=211'>212</a>\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=212'>213</a>\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=85'>86</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=86'>87</a>\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=87'>88</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=88'>89</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=89'>90</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=201'>202</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=202'>203</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=203'>204</a>\u001b[0m         batch,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=204'>205</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=205'>206</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=206'>207</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=207'>208</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=208'>209</a>\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=209'>210</a>\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=210'>211</a>\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=211'>212</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=236'>237</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=237'>238</a>\u001b[0m     \u001b[39m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=238'>239</a>\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mhandles_gradient_accumulation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=245'>246</a>\u001b[0m     \u001b[39m# -------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=246'>247</a>\u001b[0m     \u001b[39m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=247'>248</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy, block\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=248'>249</a>\u001b[0m         closure()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=250'>251</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=251'>252</a>\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=252'>253</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=253'>254</a>\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=254'>255</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=146'>147</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=147'>148</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=132'>133</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=133'>134</a>\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=135'>136</a>\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=136'>137</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:441\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=435'>436</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=436'>437</a>\u001b[0m     training_step_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39maccumulate_grad_batches\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=437'>438</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=439'>440</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_terminate_on_nan:\n\u001b[0;32m--> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=440'>441</a>\u001b[0m     check_finite_loss(result\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=442'>443</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmove_metrics_to_cpu:\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=443'>444</a>\u001b[0m     \u001b[39m# hiddens and the training step output are not moved as they are not considered \"metrics\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=444'>445</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_results \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:44\u001b[0m, in \u001b[0;36mcheck_finite_loss\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=37'>38</a>\u001b[0m \u001b[39m\"\"\"Checks for finite loss value.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=39'>40</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=40'>41</a>\u001b[0m \u001b[39m    loss: the loss value to check to be finite\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=41'>42</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misfinite(loss)\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> <a href='file:///home/huabei/anaconda3/envs/smtr2/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py?line=43'>44</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe loss returned in `training_step` is \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: The loss returned in `training_step` is nan."]}],"source":["trainer.fit(smnn, train_dataloader, val_dataloader)\n","trainer.save_checkpoint(time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) + \".ckpt\")\n","wandb.finish()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▇▇█▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_loss</td><td>7.30032</td></tr><tr><td>trainer/global_step</td><td>249</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">floral-microwave-3</strong>: <a href=\"http://localhost:8080/huabei/smtr_e3nn/runs/21rpsamu\" target=\"_blank\">http://localhost:8080/huabei/smtr_e3nn/runs/21rpsamu</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220519_104455-21rpsamu/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":[]}],"metadata":{"interpreter":{"hash":"9929d02ff7178dc29afeb4577072d89b06046f77e2112796017a63794cb707a7"},"kernelspec":{"display_name":"Python 3.8.12 ('smtr2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
